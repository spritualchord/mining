import string
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Ensure NLTK dependencies are downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Read the text from the file
with open("Preprocessing Text.txt", "r") as file:
    text = file.read()

# 1. Remove Punctuation Marks
text = text.translate(str.maketrans("", "", string.punctuation))

# 2. Tokenization
tokens = word_tokenize(text)

# 3. Remove Stop Words
stop_words = set(stopwords.words("english"))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

# 4. Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

# Recreate the processed text for TF-IDF calculation
processed_text = " ".join(stemmed_tokens)

# 5. Compute IDF using TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(use_idf=True)
tfidf_vectorizer.fit([processed_text])
idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))

# Display Results
print("\nProcessed Tokens:", stemmed_tokens)
print("\nInverse Document Frequency (IDF) Scores:")
for word, score in idf_scores.items():
    print(f"{word}: {score:.4f}")